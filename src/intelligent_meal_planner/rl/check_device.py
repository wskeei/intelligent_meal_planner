"""
æ£€æŸ¥è®­ç»ƒè®¾å¤‡é…ç½®
æŸ¥çœ‹ PyTorch æ˜¯å¦å¯ä»¥ä½¿ç”¨ GPU
"""

import sys
import io

# è®¾ç½® UTF-8 ç¼–ç ï¼Œé¿å… Windows æ§åˆ¶å°ä¹±ç 
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

import torch
import stable_baselines3 as sb3


def check_device():
    """æ£€æŸ¥å¯ç”¨çš„è®¡ç®—è®¾å¤‡"""
    print("="*60)
    print("è®­ç»ƒè®¾å¤‡é…ç½®æ£€æŸ¥")
    print("="*60)
    
    print(f"\nğŸ Python ç‰ˆæœ¬: {sys.version}")
    print(f"ğŸ”¥ PyTorch ç‰ˆæœ¬: {torch.__version__}")
    print(f"ğŸ¤– Stable-Baselines3 ç‰ˆæœ¬: {sb3.__version__}")
    
    print("\n" + "="*60)
    print("GPU å¯ç”¨æ€§æ£€æŸ¥")
    print("="*60)
    
    # æ£€æŸ¥ CUDAï¼ˆNVIDIA GPUï¼‰
    cuda_available = torch.cuda.is_available()
    print(f"\nâœ… CUDA å¯ç”¨: {cuda_available}")
    
    if cuda_available:
        print(f"   GPU æ•°é‡: {torch.cuda.device_count()}")
        print(f"   å½“å‰ GPU: {torch.cuda.current_device()}")
        print(f"   GPU åç§°: {torch.cuda.get_device_name(0)}")
        print(f"   CUDA ç‰ˆæœ¬: {torch.version.cuda}")
    else:
        print("   âš ï¸  æœªæ£€æµ‹åˆ° NVIDIA GPU æˆ– CUDA æœªå®‰è£…")
    
    # æ£€æŸ¥ MPSï¼ˆApple Siliconï¼‰
    mps_available = hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()
    print(f"\nâœ… MPS (Apple Silicon) å¯ç”¨: {mps_available}")
    
    # ç¡®å®šé»˜è®¤è®¾å¤‡
    print("\n" + "="*60)
    print("é»˜è®¤è®­ç»ƒè®¾å¤‡")
    print("="*60)
    
    if cuda_available:
        device = "cuda"
        print(f"\nğŸš€ å°†ä½¿ç”¨ GPU è®­ç»ƒï¼ˆCUDAï¼‰")
        print(f"   è¿™ä¼šå¤§å¤§åŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼")
    elif mps_available:
        device = "mps"
        print(f"\nğŸš€ å°†ä½¿ç”¨ GPU è®­ç»ƒï¼ˆApple Silicon MPSï¼‰")
        print(f"   è¿™ä¼šå¤§å¤§åŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼")
    else:
        device = "cpu"
        print(f"\nğŸ’» å°†ä½¿ç”¨ CPU è®­ç»ƒ")
        print(f"   âš ï¸  è®­ç»ƒé€Ÿåº¦è¾ƒæ…¢ï¼Œä½†å®Œå…¨å¯ä»¥ä½¿ç”¨")
    
    print(f"\n   é»˜è®¤è®¾å¤‡: {device}")
    
    # æ€§èƒ½å»ºè®®
    print("\n" + "="*60)
    print("è®­ç»ƒæ€§èƒ½å»ºè®®")
    print("="*60)
    
    if device == "cpu":
        print("\nğŸ“Š CPU è®­ç»ƒå»ºè®®ï¼š")
        print("   â€¢ å¿«é€Ÿè®­ç»ƒï¼ˆæµ‹è¯•ï¼‰ï¼š1ä¸‡æ­¥ çº¦ 5-10 åˆ†é’Ÿ")
        print("   â€¢ æ ‡å‡†è®­ç»ƒï¼š10ä¸‡æ­¥ çº¦ 30-60 åˆ†é’Ÿ")
        print("   â€¢ å®Œæ•´è®­ç»ƒï¼š50ä¸‡æ­¥ çº¦ 2-5 å°æ—¶")
        print("\nğŸ’¡ æç¤ºï¼š")
        print("   â€¢ CPU è®­ç»ƒå®Œå…¨å¯è¡Œï¼Œè¿™ä¸ªé¡¹ç›®æ¨¡å‹å¾ˆå°")
        print("   â€¢ å¦‚æœæœ‰ NVIDIA GPUï¼Œå¯ä»¥å®‰è£… CUDA ç‰ˆæœ¬çš„ PyTorch")
        print("   â€¢ å‘½ä»¤ï¼špip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
    else:
        print("\nğŸ“Š GPU è®­ç»ƒå»ºè®®ï¼š")
        print("   â€¢ å¿«é€Ÿè®­ç»ƒï¼š1ä¸‡æ­¥ çº¦ 1-2 åˆ†é’Ÿ")
        print("   â€¢ æ ‡å‡†è®­ç»ƒï¼š10ä¸‡æ­¥ çº¦ 10-20 åˆ†é’Ÿ")
        print("   â€¢ å®Œæ•´è®­ç»ƒï¼š50ä¸‡æ­¥ çº¦ 1-2 å°æ—¶")
        print("\nğŸ’¡ GPU åŠ é€Ÿæ•ˆæœæ˜¾è‘—ï¼Œè®­ç»ƒé€Ÿåº¦å¿« 5-10 å€ï¼")
    
    print("\n" + "="*60)
    print("Stable-Baselines3 è®¾å¤‡è¯´æ˜")
    print("="*60)
    
    print("\nğŸ“š å…³äº Stable-Baselines3 çš„è®¾å¤‡ä½¿ç”¨ï¼š")
    print("   â€¢ SB3 åŸºäº PyTorch æ„å»º")
    print("   â€¢ è‡ªåŠ¨æ£€æµ‹å¯ç”¨è®¾å¤‡ï¼ˆGPU > CPUï¼‰")
    print("   â€¢ DQN æ¨¡å‹ä¼šè‡ªåŠ¨ä½¿ç”¨ PyTorch çš„é»˜è®¤è®¾å¤‡")
    print("   â€¢ æ— éœ€æ‰‹åŠ¨æŒ‡å®šè®¾å¤‡ï¼ŒSB3 ä¼šæ™ºèƒ½é€‰æ‹©")
    
    print("\nâœ¨ å¦‚æœä½ æƒ³å¼ºåˆ¶ä½¿ç”¨ç‰¹å®šè®¾å¤‡ï¼š")
    print("   å¯ä»¥åœ¨åˆ›å»ºæ¨¡å‹æ—¶ä¼ å…¥ device å‚æ•°ï¼š")
    print("   model = DQN('MlpPolicy', env, device='cuda')  # å¼ºåˆ¶ä½¿ç”¨ GPU")
    print("   model = DQN('MlpPolicy', env, device='cpu')   # å¼ºåˆ¶ä½¿ç”¨ CPU")
    
    print("\n" + "="*60)
    
    return device


if __name__ == "__main__":
    device = check_device()
    print(f"\nå½“å‰ç³»ç»Ÿå°†é»˜è®¤ä½¿ç”¨: {device.upper()}")
    print("="*60)