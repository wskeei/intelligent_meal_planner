# 基于强化学习与多Agent协作的智能配餐系统

## 1. 项目概述

### 1.1. 项目背景与问题

在快节奏的现代生活中，如何规划每日膳食以满足个性化的健康需求（如健身增肌、减脂、疾病管理）、预算限制和口味偏好，已成为一个普遍的难题。传统的人工配餐方式耗时耗力，且难以兼顾多个优化目标，而市面上的配餐软件大多提供固定的模板，缺乏真正的个性化和动态适应能力。

### 1.2. 解决方案

本项目旨在研发一个**基于强化学习（Reinforcement Learning）与多 Agent 协作的智能配餐系统**。该系统通过模拟一个“智能营养师团队”，能够与用户进行深度对话，理解其复杂、个性化的需求，并利用强化学习算法，动态地为用户生成最优的一日三餐食谱推荐。

### 1.3. 核心技术与创新点

*   **算法创新**：以**深度Q网络（DQN）**这一前沿的强化学习算法作为核心，解决传统配餐问题中的多目标优化难题，体现了系统的“智能决策”能力。
*   **架构创新**：采用**`CrewAI`多 Agent 框架**，将复杂的任务分解给“用户需求分析师”和“强化学习配餐师”两个角色协同完成，模拟了专家团队的工作流，提升了系统的交互智能和模块化程度。
*   **应用创新**：将先进的 AI 技术应用于“个性化健康饮食”这一贴近日常生活的场景，具有很高的实用价值和良好的可演示性。

---

## 2. 系统架构设计

系统整体采用前后端分离架构，核心逻辑由后端的多 Agent 系统驱动。

```mermaid
graph TD
    subgraph "前端 (User Interface)"
        UI[用户交互界面<br/>(Streamlit / HTML+JS)]
    end

    subgraph "后端 (Backend Server)"
        API[API 服务<br/>(FastAPI)]
        subgraph "CrewAI: 智能营养师团队"
            Agent1[用户需求分析师 Agent]
            Agent2[强化学习配餐师 Agent]
            Agent1 -- 结构化需求(JSON) --> Agent2
        end
        subgraph "核心算法模块"
            RL_Model[训练好的DQN模型]
            RecipeDB[菜品数据库 (recipes.json)]
        end
        Agent2 -- 调用 --> RL_Model
        Agent2 -- 查询 --> RecipeDB
    end

    UI -- HTTP请求 --> API
    API -- 触发 --> Agent1
    API -- 返回最终食谱 --> UI
```

### 2.1. 前端 (User Interface)

*   **功能**：提供一个简洁友好的界面，供用户与“用户需求分析师 Agent”进行对话，输入自己的饮食需求，并最终展示图文并茂的配餐结果。
*   **技术选型**：**Streamlit** 或 **Gradio** (快速原型开发) / **Vue.js + Element UI** (更精美的界面)。

### 2.2. 后端 (Backend Server)

*   **API 服务**: 使用 **FastAPI** 框架，将 CrewAI 的工作流封装成一个 API 接口，接收前端请求并返回结果。
*   **CrewAI 智能营养师团队**:
    *   **用户需求分析师 (User Profiler Agent)**：负责与用户进行引导式对话，全面捕获其健康目标、预算、口味偏好、忌口等信息，并将其整理成一份结构化的 **JSON** 报告。
    *   **强化学习配餐师 (RL Chef Agent)**：接收 JSON 报告，根据报告内容**动态配置**强化学习环境的奖励函数。随后，它调用一个封装好的 **`RL_Model_Tool`** 工具来运行已训练的 DQN 模型，获取最优的菜品ID组合。最后，通过 **`Recipe_Database_Tool`** 查询菜品详情，生成最终的推荐文案。
*   **核心算法模块**:
    *   **菜品数据库 (`recipes.json`)**: 项目的“世界规则”，一个包含约200道菜品的 JSON 文件，详细记录了每道菜的营养成分、价格、标签等信息。**这是项目唯一需要准备的数据**。
    *   **训练好的 DQN 模型**: 这是通过**离线训练**得到的模型文件。它包含了从成千上万次模拟配餐中学习到的“配餐智慧”。

---

## 3. 核心算法：强化学习配餐模型

我们将配餐问题建模为一个马尔可夫决策过程 (MDP)，并使用 DQN 算法进行求解。

*   **状态 (State)**：一个元组，表示 `(当前是第几餐, 已摄入卡路里, 已摄入蛋白质, ..., 已花费预算)`。
*   **动作 (Action)**：从菜品数据库中选择一道符合当前餐别（早餐/午餐/晚餐）的菜品。
*   **奖励函数 (Reward Function)**：在一天配餐结束后（完成三餐选择后）计算。这是一个关键的设计，它会是一个综合评分：
    *   `R_total = w1*R_nutrition + w2*R_budget + w3*R_variety + P_dislike`
    *   `R_nutrition`: 营养达标奖励。总营养素与目标的差距越小，奖励越高。可以使用高斯函数或分段函数实现。
    *   `R_budget`: 预算奖励。未超预算则为0，超出预算则给予一个较大的负值（惩罚）。
    *   `R_variety`: 多样性奖励。如果三餐菜品属于不同类别，给予少量正奖励。
    *   `P_dislike`: 忌口惩罚。如果食谱中包含用户忌口的菜品，给予一个极大的负值。
    *   `w1, w2, w3`: 可调权重，用于平衡不同目标的重要性。

---

## 4. 具体实现思路与步骤

### 第一阶段：核心算法模块搭建 (离线)

1.  **创建菜品数据库**: 手动收集100-200道菜品信息，整理成 `recipes.json` 文件。
2.  **构建强化学习环境**: 使用 Python 创建一个 `MealPlanningEnv` 类，该类需要遵循 OpenAI Gym (现 Gymnasium) 的标准接口，包含 `reset()`, `step()`, `render()` 等方法。`step()` 方法内部会实现奖励函数的计算逻辑。
3.  **训练 DQN 模型**:
    *   选用一个成熟的强化学习库，如 **`Stable-Baselines3`** 或 **`PyTorch`**。
    *   定义 DQN 模型的网络结构（一个简单的多层感知机即可）。
    *   编写训练脚本，在 `MealPlanningEnv` 环境中对 DQN Agent 进行成千上万次的回合 (Episode) 训练。
    *   将训练好的模型权重保存为文件（例如 `dqn_meal_planner.zip`）。

### 第二阶段：后端 Agent 系统开发

4.  **封装工具 (Tools)**:
    *   创建 `RL_Model_Tool`：加载并调用已训练的 DQN 模型，输入当前状态，输出最优动作。
    *   创建 `Recipe_Database_Tool`：根据菜品ID查询 `recipes.json`，返回详细信息。
5.  **开发 CrewAI Agents**:
    *   编写 `User Profiler Agent` 的 Prompt 和角色定义。
    *   编写 `RL Chef Agent` 的 Prompt 和角色定义，并为其分配上一步创建的两个工具。
6.  **搭建 FastAPI 服务**: 创建一个 `/plan_meal` 的 API 接口，该接口接收用户的初始对话，启动 CrewAI 工作流，并等待其返回最终结果。

### 第三阶段：前端界面开发与联调

7.  **设计前端界面**: 使用 Streamlit 或其他前端框架，设计一个包含对话窗口和结果展示区的用户界面。
8.  **前后端联调**: 前端通过 HTTP 请求调用后端的 `/plan_meal` 接口，并将返回的图文并茂的食谱展示给用户。
